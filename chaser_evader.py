# -*- coding: utf-8 -*-
"""Chaser-Evader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HGDAkX07BCaJu_LPh4flR5arWJ4Dhnq6
"""

!pip install gym matplotlib stable-baselines3

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

pwd

# Commented out IPython magic to ensure Python compatibility.
# %%writefile chase_evade_simple_env.py
# 
# import gym
# from gym import spaces
# import numpy as np
# 
# class ChaseEvadeSimpleEnv(gym.Env):
#     """
#     A lightweight Gym-compatible environment for 2D chase-and-evade.
#     One agent (chaser) is trained using RL. The evader uses a simple heuristic.
#     """
#     metadata = {'render.modes': ['human']}
# 
#     def __init__(self):
#         super(ChaseEvadeSimpleEnv, self).__init__()
#         self.grid_size = 10.0
#         self.catch_distance = 1.0
#         self.max_steps = 3000
#         self.step_size = 0.3
#         self.step_count = 0
# 
#         self.observation_space = spaces.Box(
#             low=0.0, high=self.grid_size, shape=(4,), dtype=np.float32
#         )
# 
#         self.action_space = spaces.Box(
#             low=-1.0, high=1.0, shape=(2,), dtype=np.float32
#         )
# 
#         self.chaser_pos = np.zeros(2)
#         self.evader_pos = np.zeros(2)
# 
#     def reset(self):
#         self.chaser_pos = np.random.uniform(0, self.grid_size, size=2)
#         self.evader_pos = np.random.uniform(0, self.grid_size, size=2)
#         self.step_count = 0
#         return self._get_obs()
# 
#     def _get_obs(self):
#         return np.concatenate([self.chaser_pos, self.evader_pos]).astype(np.float32)
# 
#     def step(self, action):
#         self.step_count += 1
# 
#         action = np.clip(action, -1.0, 1.0)
#         self.chaser_pos = np.clip(self.chaser_pos + action * self.step_size, 0, self.grid_size)
# 
#         # Evader moves away from chaser
#         direction = self.evader_pos - self.chaser_pos
#         norm = np.linalg.norm(direction)
#         if norm > 0:
#             evader_move = (direction / norm) * self.step_size
#             self.evader_pos = np.clip(self.evader_pos + evader_move, 0, self.grid_size)
# 
#         distance = np.linalg.norm(self.chaser_pos - self.evader_pos)
# 
#         done = False
#         reward = -1.0  # time penalty
#         if distance < self.catch_distance:
#             reward = 10.0
#             done = True
#         elif self.step_count >= self.max_steps:
#             done = True
# 
#         obs = self._get_obs()
#         info = {"step": self.step_count, "distance": distance}
#         return obs, reward, done, info
# 
#     def render(self, mode='human'):
#         print(f"Step {self.step_count} | Chaser: {self.chaser_pos} | Evader: {self.evader_pos}")
# 
#     def close(self):
#         pass
#



from chase_evade_simple_env import ChaseEvadeSimpleEnv
from stable_baselines3 import PPO
from stable_baselines3 import SAC

!pip install Shimmy

ls

#model = PPO.load("ppo_chaser")

# Step 2: Create a new environment with updated rewards
#env = ChaseEvadeSimpleEnv(record_positions=False)  # updated reward logic in step()

# Step 3: Continue training with new reward logic
#model.set_env(env)
#model.learn(total_timesteps=400_000, reset_num_timesteps=False)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ChaseEvadeShapedEnv.py
# import gym
# from gym import spaces
# import numpy as np
# 
# class ChaseEvadeSimpleEnv(gym.Env):
#    """
#    A lightweight Gym-compatible environment for 2D chase-and-evade.
#    One agent (chaser) is trained using RL. The evader uses a simple heuristic.
#    """
#    metadata = {'render.modes': ['human']}
# 
#    def __init__(self):
#        super(ChaseEvadeSimpleEnv, self).__init__()
#        self.grid_size = 10.0
#        self.catch_distance = 1.0
#        self.max_steps = 3000
#        self.step_size = 0.3
#        self.step_count = 0
# 
#        self.observation_space = spaces.Box(
#            low=0.0, high=self.grid_size, shape=(4,), dtype=np.float32
#        )
# 
#        self.action_space = spaces.Box(
#            low=-1.0, high=1.0, shape=(2,), dtype=np.float32
#        )
# 
#        self.chaser_pos = np.zeros(2)
#        self.evader_pos = np.zeros(2)
# 
#    def reset(self):
#        self.chaser_pos = np.random.uniform(0, self.grid_size, size=2)
#        self.evader_pos = np.random.uniform(0, self.grid_size, size=2)
#        self.step_count = 0
#        return self._get_obs()
# 
#    def _get_obs(self):
#        return np.concatenate([self.chaser_pos, self.evader_pos]).astype(np.float32)
# 
#    def step(self, action):
#        self.step_count += 1
# 
#        action = np.clip(action, -1.0, 1.0)
#        self.chaser_pos = np.clip(self.chaser_pos + action * self.step_size, 0, self.grid_size)
# 
#        # Evader moves away from chaser
#        direction = self.evader_pos - self.chaser_pos
#        norm = np.linalg.norm(direction)
#        if norm > 0:
#            evader_move = (direction / norm) * self.step_size
#            self.evader_pos = np.clip(self.evader_pos + evader_move, 0, self.grid_size)
# 
#        distance = np.linalg.norm(self.chaser_pos - self.evader_pos)
# 
#        done = False
#        reward = -0.1 * distance  # time penalty
# 
#        if distance < 2.0:
#            reward += 0.5  # small bonus for getting near
# 
#        if distance < self.catch_distance:
#            reward += 10.0
#            done = True
#        elif self.step_count >= self.max_steps:
#            done = True
# 
#        obs = self._get_obs()
#        info = {"step": self.step_count, "distance": distance}
#        return obs, reward, done, info
# 
#    def render(self, mode='human'):
#        print(f"Step {self.step_count} | Chaser: {self.chaser_pos} | Evader: {self.evader_pos}")
# 
#    def close(self):
#        pass

from chase_evade_simple_env import ChaseEvadeSimpleEnv
from stable_baselines3 import PPO
from stable_baselines3 import SAC

env = ChaseEvadeSimpleEnv()
#model = PPO("MlpPolicy", env, verbose=1)
model = SAC("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100000)
model.save("sac_chaser")

model.print

model.save("ppo_chaser")

from chase_evade_simple_env import ChaseEvadeSimpleEnv
from stable_baselines3 import PPO

# Re-create the environment (should match training env setup)
env = ChaseEvadeSimpleEnv()

# Load the previously saved model and attach the environment
model = PPO.load("ppo_chaser", env=env, verbose=1)

# Continue training for 500,000 more timesteps
model.learn(total_timesteps=500_000)

# Save the continued model
model.save("ppo_chaser_continued")

from chase_evade_simple_env import ChaseEvadeSimpleEnv
#from stable_baselines3 import PPO
from stable_baselines3 import SAC
import numpy as np

# Load environment and model
env = ChaseEvadeSimpleEnv()
model = SAC.load("sac_chaser")

chaser_path = []
evader_path = []

obs = env.reset()
done = False

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, info = env.step(action)

    # Save positions for animation
    chaser_path.append(env.chaser_pos.copy())
    evader_path.append(env.evader_pos.copy())

print(f"First chaser step: {chaser_path[0]}")
print(f"First evader step: {evader_path[0]}")
print(f"Types: {type(chaser_path[0][0])}, {type(evader_path[0][0])}")

import matplotlib.pyplot as plt
import matplotlib.animation as animation
import matplotlib as mpl
mpl.rcParams['animation.embed_limit'] = 50

# Make sure there's something to animate
if len(chaser_path) > 0 and len(evader_path) > 0:
    fig, ax = plt.subplots()
    ax.set_xlim(0, env.grid_size)
    ax.set_ylim(0, env.grid_size)
    ax.set_title("Chaser vs Evader")
    chaser_dot, = ax.plot([], [], 'ro', label='Chaser')
    evader_dot, = ax.plot([], [], 'bo', label='Evader')
    #ax.legend()

    def init():
        chaser_dot.set_data([], [])
        evader_dot.set_data([], [])
        return chaser_dot, evader_dot

    def update(frame):
        cx, cy = chaser_path[frame]
        ex, ey = evader_path[frame]
        chaser_dot.set_data([float(cx)], [float(cy)])
        evader_dot.set_data([float(ex)], [float(ey)])
        return chaser_dot, evader_dot

    ani = animation.FuncAnimation(
        fig, update, frames=len(chaser_path),
        init_func=init, blit=True, interval=50, repeat=False
    )

    from IPython.display import HTML
    HTML(ani.to_jshtml())  # To display inside notebook
    ani.save("chase_evade_eval.gif", writer="pillow")
    print("✅ GIF saved as chase_evade_eval.gif")
else:
    print("❌ Not enough steps to animate!")

import matplotlib.pyplot as plt
import matplotlib.animation as animation
import matplotlib as mpl
mpl.rcParams['animation.embed_limit'] = 50

if len(chaser_path) > 0 and len(evader_path) > 0:
    fig, ax = plt.subplots()
    ax.set_xlim(0, env.grid_size)
    ax.set_ylim(0, env.grid_size)
    ax.set_title("Chaser vs Evader", color='white')
    fig.patch.set_facecolor('black')
    ax.set_facecolor('black')
    ax.axis('off')  # Hide axes

    # Main dots (chaser and evader)
    chaser_dot, = ax.plot([], [], 'ro', markersize=8)
    evader_dot, = ax.plot([], [], 'bo', markersize=8)

    # Trails (line plots)
    chaser_trail, = ax.plot([], [], 'r-', linewidth=1)
    evader_trail, = ax.plot([], [], 'b-', linewidth=1)

    def init():
        chaser_dot.set_data([], [])
        evader_dot.set_data([], [])
        chaser_trail.set_data([], [])
        evader_trail.set_data([], [])
        return chaser_dot, evader_dot, chaser_trail, evader_trail

    def update(frame):
        # Current positions
        cx, cy = chaser_path[frame]
        ex, ey = evader_path[frame]
        chaser_dot.set_data([float(cx)], [float(cy)])
        evader_dot.set_data([float(ex)], [float(ey)])

        # Trail paths up to current frame
        chaser_trail.set_data(
            [float(p[0]) for p in chaser_path[:frame + 1]],
            [float(p[1]) for p in chaser_path[:frame + 1]]
        )
        evader_trail.set_data(
            [float(p[0]) for p in evader_path[:frame + 1]],
            [float(p[1]) for p in evader_path[:frame + 1]]
        )
        return chaser_dot, evader_dot, chaser_trail, evader_trail

    ani = animation.FuncAnimation(
        fig, update, frames=len(chaser_path),
        init_func=init, blit=True, interval=50, repeat=False
    )

    # Save and optionally display in notebook
    from IPython.display import HTML
    HTML(ani.to_jshtml())
    ani.save("chase_evade_eval_trail.gif", writer="pillow")
    print("✅ Enhanced GIF saved as chase_evade_eval_trail.gif")

else:
    print("❌ Not enough steps to animate!")